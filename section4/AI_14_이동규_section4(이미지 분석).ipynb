{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SbeJiIbsa5SN"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_style('darkgrid')\n",
        "\n",
        "import os\n",
        "from glob import glob\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.applications.densenet import DenseNet121\n",
        "from keras.layers import Dense, GlobalAveragePooling2D\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "from keras.models import load_model\n",
        "\n",
        "from itertools import chain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#파일 불러오기\n",
        "df = pd.read_csv('/content/drive/MyDrive/AI/section4/project/chunan.csv', index_col=0)\n",
        "num_obs = len(df)\n",
        "print('Number of observations in dataset:',num_obs)\n",
        "\n",
        "# 이미지 경로\n",
        "my_glob = glob('/content/drive/MyDrive/AI/section4/project/chunan/*.jpg')\n",
        "print('Number of observations in imagefolders:', len(my_glob))\n",
        "\n",
        "# csv파일에 경로 붙이기\n",
        "full_img_paths = {os.path.basename(x): x for x in my_glob}\n",
        "df['full_path'] = df['Id'].map(full_img_paths.get)\n",
        "\n",
        "# 예상육량에 대해 one hot encoding\n",
        "from itertools import chain\n",
        "all_labels = np.unique(list(chain(*df['예상육량'].map(lambda x: x.split('|')).tolist())))\n",
        "all_labels = [x for x in all_labels if len(x)>0]\n",
        "for c_label in all_labels:\n",
        "    if len(c_label)>1: # 라벨 중 0값 제외\n",
        "        df[c_label] = df['예상육량'].map(lambda finding: 1 if c_label in finding else 0)\n"
      ],
      "metadata": {
        "id": "bJdLI1KabLya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx = df['월령'].sort_values().index[-2]\n",
        "plt.imshow(plt.imread(df.iloc[idx]['full_path']), cmap='bone')"
      ],
      "metadata": {
        "id": "uTSVPPIwcL2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(df['예상육질'])"
      ],
      "metadata": {
        "id": "JRsoUJ3rcqKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sns.countplot()\n",
        "sns.barplot(x=all_labels, \n",
        "            y=df[all_labels].sum(), \n",
        "            order = df[all_labels].sum().sort_values(ascending=False).index)\n",
        "\n",
        "plt.tick_params(axis='x')"
      ],
      "metadata": {
        "id": "t0BBK7h_c7Mh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "n = None # use all data\n",
        "gss = GroupShuffleSplit(n_splits=1, train_size=.7, random_state=42)\n",
        "\n",
        "for train_idx, test_idx in gss.split(df[: n], groups = df[: n]['월령'].values):\n",
        "    train_df = df.iloc[train_idx]\n",
        "    test_df, valid_df = train_test_split(df.iloc[test_idx], \n",
        "                                   test_size = 0.2, \n",
        "                                   random_state = 42) #should add stratified sampling\n",
        "    \n",
        "train_df.head()\n",
        "test_df.head()"
      ],
      "metadata": {
        "id": "sNS7mYuQdIgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_train_generator(df, image_dir, x_col, y_cols, shuffle=True, batch_size=32, seed=1, target_w = 320, target_h = 320):\n",
        "   \n",
        "    print(\"getting train generator...\") \n",
        "    # 이미지 정규화\n",
        "    image_generator = ImageDataGenerator(\n",
        "        samplewise_center=True,\n",
        "        samplewise_std_normalization= True)\n",
        "    \n",
        "    # 이미지 경로에 특정 batch size 적용 \n",
        "    generator = image_generator.flow_from_dataframe(\n",
        "            dataframe=df,\n",
        "            directory=image_dir,\n",
        "            x_col=x_col,\n",
        "            y_col=y_cols,\n",
        "            class_mode=\"raw\",\n",
        "            batch_size=batch_size,\n",
        "            shuffle=shuffle,\n",
        "            seed=seed,\n",
        "            target_size=(target_w,target_h))\n",
        "    \n",
        "    return generator\n",
        "\n",
        "def get_test_and_valid_generator(valid_df, test_df, train_df, image_dir, x_col, y_cols, sample_size=100, batch_size=32, seed=1, target_w = 320, target_h = 320):\n",
        "    \n",
        "    print(\"getting train and valid generators...\")\n",
        "    # 데이터 셋에서 generator 적용\n",
        "    raw_train_generator = ImageDataGenerator().flow_from_dataframe(\n",
        "        dataframe=train_df, \n",
        "        directory=IMAGE_DIR, \n",
        "        x_col=x_col, \n",
        "        y_col=y_cols, \n",
        "        class_mode=\"raw\", \n",
        "        batch_size=sample_size, \n",
        "        shuffle=True, \n",
        "        target_size=(target_w, target_h))\n",
        "    # get data sample\n",
        "    batch = raw_train_generator.next()\n",
        "    data_sample = batch[0]\n",
        "\n",
        "    # 테스트 generator를 위한 평균 및 표준편차 적용\n",
        "    image_generator = ImageDataGenerator(\n",
        "        featurewise_center=True,\n",
        "        featurewise_std_normalization= True)\n",
        "    \n",
        "    # generator를 훈련 데이터에 적용\n",
        "    image_generator.fit(data_sample)\n",
        "\n",
        "    # generator를 test 데이터에 적용\n",
        "    valid_generator = image_generator.flow_from_dataframe(\n",
        "            dataframe=valid_df,\n",
        "            directory=image_dir,\n",
        "            x_col=x_col,\n",
        "            y_col=y_cols,\n",
        "            class_mode=\"raw\",\n",
        "            batch_size=batch_size,\n",
        "            shuffle=False,\n",
        "            seed=seed,\n",
        "            target_size=(target_w,target_h))\n",
        "\n",
        "    test_generator = image_generator.flow_from_dataframe(\n",
        "            dataframe=test_df,\n",
        "            directory=image_dir,\n",
        "            x_col=x_col,\n",
        "            y_col=y_cols,\n",
        "            class_mode=\"raw\",\n",
        "            batch_size=batch_size,\n",
        "            shuffle=False,\n",
        "            seed=seed,\n",
        "            target_size=(target_w,target_h))\n",
        "    return valid_generator, test_generator       \n"
      ],
      "metadata": {
        "id": "tyH6gsTKdica"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_DIR = None # 모든 행에 이미지 경로 추가\n",
        "train_generator = get_train_generator(train_df, IMAGE_DIR, \"full_path\", all_labels)\n",
        "valid_generator, test_generator= get_test_and_valid_generator(valid_df, test_df, train_df, IMAGE_DIR, \"full_path\", all_labels)"
      ],
      "metadata": {
        "id": "fFnK69pteWY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x, y = train_generator.__getitem__(2)\n",
        "plt.imshow(x[0]);"
      ],
      "metadata": {
        "id": "FTX1IrbPehNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_class_freqs(labels):\n",
        "    \n",
        "    # 개체의 예상육량의 개수\n",
        "    N = len(labels)\n",
        "    \n",
        "    positive_frequencies = (np.sum(labels, 0)) / N\n",
        "    negative_frequencies = (1- positive_frequencies)\n",
        "\n",
        "    return positive_frequencies, negative_frequencies\n",
        "\n",
        "\n",
        "\n",
        "def get_weighted_loss(pos_weights, neg_weights, epsilon=1e-7):\n",
        "    \n",
        "    def weighted_loss(y_true, y_pred):\n",
        "\n",
        "        # 손실함수 초기화\n",
        "        loss = 0.0\n",
        "\n",
        "        for i in range(len(pos_weights)):\n",
        "            # 특성에 평균 가중치 적용\n",
        "            loss += -1 * K.mean(pos_weights * y_true * K.log(y_pred + epsilon) + \n",
        "                          (1 - y_true) * neg_weights * K.log(1 - y_pred + epsilon))\n",
        "            \n",
        "        return loss\n",
        "    \n",
        "    return weighted_loss\n",
        "\n",
        "freq_pos, freq_neg = compute_class_freqs(train_generator.labels)\n",
        "pos_weights = freq_neg\n",
        "neg_weights = freq_pos\n",
        "pos_contribution = freq_pos * pos_weights \n",
        "neg_contribution = freq_neg * neg_weights"
      ],
      "metadata": {
        "id": "BlUJjwp3elgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 예상육량 불균형 균형화\n",
        "data = pd.DataFrame({\"Class\": all_labels, \"Label\": \"Positive\", \"Value\": pos_contribution})\n",
        "data = data.append([{\"Class\": all_labels[l], \"Label\": \"Negative\", \"Value\": v} \n",
        "                        for l,v in enumerate(neg_contribution)], ignore_index=True)\n",
        "\n",
        "sns.barplot(x=\"Class\", y=\"Value\", hue=\"Label\" ,data=data);"
      ],
      "metadata": {
        "id": "OOmLZFbhfFXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = DenseNet121(\n",
        "    include_top=False,\n",
        "    weights=\"imagenet\",\n",
        "    input_tensor=None,\n",
        "    input_shape=None,\n",
        ")\n",
        "layer_names = [layer.name for layer in base_model.layers]\n",
        "layers = [base_model.get_layer(name).output for name in layer_names]\n",
        "\n",
        "\n",
        "down_stack = Model(inputs=base_model.input, outputs=layers)\n",
        "\n",
        "down_stack.trainable = False    \n",
        "\n",
        "x = base_model.output\n",
        "\n",
        "# global spatial average pooling layer 추가\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "# 층 추가\n",
        "predictions = Dense(len(all_labels), activation=\"sigmoid\")(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "model.compile(metrics=['accuracy'] ,optimizer='adam', loss=get_weighted_loss(pos_weights, neg_weights))\n",
        "\n",
        "history = model.fit(train_generator, \n",
        "                    validation_data=valid_generator,\n",
        "                    steps_per_epoch=5, \n",
        "                    validation_steps=100, \n",
        "                    epochs = 20)"
      ],
      "metadata": {
        "id": "6n1ngB4ZfUZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow\n",
        "tensorflow.keras.utils.plot_model(model, show_shapes=True)"
      ],
      "metadata": {
        "id": "-zLwl72GBwHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.ylabel(\"loss\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.title(\"Training Loss Curve\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "F9sjXhia1ZEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_Y = model.predict(test_generator, batch_size = 25, verbose = True)"
      ],
      "metadata": {
        "id": "x09BL2bN35jo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_generator, \n",
        "                    validation_data=test_generator,\n",
        "                    steps_per_epoch=5, \n",
        "                    validation_steps=100, \n",
        "                    epochs = 20)"
      ],
      "metadata": {
        "id": "eCBEVAEQCAfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.ylabel(\"loss\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.title(\"Training Loss Curve\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "D97MfTtcDgt-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "test_Y = test_generator.labels\n",
        "fig, c_ax = plt.subplots(1,1, figsize = (9, 9))\n",
        "\n",
        "for (idx, c_label) in enumerate(all_labels):\n",
        "    fpr, tpr, thresholds = roc_curve(test_Y[:,idx].astype(int), pred_Y[:,idx])\n",
        "    c_ax.plot(fpr, tpr, label = '%s (AUC:%0.2f)'  % (c_label, auc(fpr, tpr)))\n",
        "\n",
        "c_ax.legend()\n",
        "c_ax.set_xlabel('False Positive Rate')\n",
        "c_ax.set_ylabel('True Positive Rate')"
      ],
      "metadata": {
        "id": "RbClh5Zp392X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for c_label, p_count, t_count in zip(all_labels, \n",
        "                                     100*np.mean(pred_Y,0), \n",
        "                                     100*np.mean(test_Y,0)):\n",
        "    print('%s: 등급: %2.2f%%, predict 등급: %2.2f%%' % (c_label, t_count, p_count))"
      ],
      "metadata": {
        "id": "kCSYXmvjHkP-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}